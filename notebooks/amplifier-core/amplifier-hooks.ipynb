{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amplifier Hooks: The Agent's Nervous System\n",
    "\n",
    "This notebook explains the Amplifier hooks system - how hooks provide observability, control, and extensibility without modifying core code.\n",
    "\n",
    "## What Are Hooks?\n",
    "\n",
    "**Hooks** are observability and extension points in the Amplifier execution flow. They allow you to:\n",
    "\n",
    "- **Observe**: See what's happening during execution\n",
    "- **Log**: Record events for debugging and auditing\n",
    "- **Modify**: Alter behavior through interception\n",
    "- **Control**: Add approval gates and safety checks\n",
    "- **Extend**: Add new capabilities without core changes\n",
    "\n",
    "### Philosophy\n",
    "\n",
    "Hooks embody the **\"observability mechanism\"** principle:\n",
    "- Kernel emits lifecycle events\n",
    "- Hooks react to events\n",
    "- Multiple hooks compose together\n",
    "- No core code modifications needed\n",
    "\n",
    "Think of it as: **Hooks are the agent's nervous system - sensing and responding to execution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Event Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualizing the Hook Lifecycle\n\nThe hook lifecycle follows these stages:\n\n1. **Hook Mounted** - Module registers event handlers\n2. **Event Emitted** - Core component fires lifecycle event\n3. **Hook Receives** - All registered handlers called in order\n4. **Hook Reacts** - Handler executes (log, modify, approve, etc.)\n5. **Continue** - Execution continues normally\n\n### Example Event Flow\n\n```\nUser: 'Read test.txt'\n  â†“\nOrchestrator: Calls LLM\n  â†“\nLLM: Returns tool call read_file(path='test.txt')\n  â†“\nðŸ“¡ Event: tool:invoked {tool_name: 'read_file', path: 'test.txt'}\n  â†“ (hooks listening)\n  â”œâ”€ hooks-logging: Logs 'Tool invoked: read_file'\n  â”œâ”€ hooks-approval: Asks 'Allow reading test.txt? [y/n]'\n  â””â”€ hooks-backup: Saves state\n  â†“\nTool: Executes read_file\n  â†“\nðŸ“¡ Event: tool:completed {tool_name: 'read_file', result: '...'}\n  â†“ (hooks listening)\n  â”œâ”€ hooks-logging: Logs 'Tool completed: read_file âœ“'\n  â””â”€ hooks-backup: Saves updated state\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Available Hooks Overview\n",
    "\n",
    "Amplifier provides several built-in hooks, each serving a specific purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Amplifier Hooks Catalog\n\n**Observability**\n- **hooks-logging** - Log all execution events\n  - When: Always (debugging, monitoring)\n  - Events: All events\n  - Key Config: `level: INFO|DEBUG|WARNING`, `output: console|file`\n\n**Safety**\n- **hooks-backup** - Automatic conversation backup\n  - When: Production (recovery)\n  - Events: Periodic, session:end\n  - Key Config: `interval: 300`, `backup_dir: .amplifier/backups`\n\n- **hooks-approval** - Human approval for high-risk ops\n  - When: High-risk operations\n  - Events: tool:invoked\n  - Key Config: `timeout: 300`, `auto_approve: false`\n\n**Security**\n- **hooks-redaction** - Redact secrets from logs\n  - When: Production (compliance)\n  - Events: All logged events\n  - Key Config: `patterns: [\"API key regex\"]`, `replacement: \"[REDACTED]\"`\n\n**Optimization**\n- **hooks-scheduler-cost-aware** - Cost-based provider routing\n  - When: Budget-constrained\n  - Events: llm:request\n  - Key Config: `daily_budget: 10.0`, `prefer_cheaper: true`\n\n- **hooks-scheduler-heuristic** - Task-based provider selection\n  - When: Multi-provider setups\n  - Events: llm:request\n  - Key Config: `rules: \"task_type â†’ provider\"`\n\n**Monitoring**\n- **hooks-status-context** - Track context usage\n  - When: Long conversations\n  - Events: context:usage\n  - Key Config: `warn_threshold: 0.80`, `alert_threshold: 0.90`\n\n**UX**\n- **hooks-streaming-ui** - Enhanced streaming display\n  - When: Interactive CLI/web\n  - Events: llm:response:stream\n  - Key Config: `render_markdown: true`, `highlight_code: true`\n\n**Productivity**\n- **hooks-todo-reminder** - Todo list management\n  - When: Multi-step tasks\n  - Events: Periodic checks\n  - Key Config: `remind_interval: 5`, `suggest_threshold: 3`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: hooks-logging - The Essential Hook\n",
    "\n",
    "The most important hook is `hooks-logging`. It provides visibility into everything happening during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating hooks-logging behavior\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LoggingHook:\n",
    "    \"\"\"Simplified version of hooks-logging for demonstration.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict[str, Any]):\n",
    "        self.level = config.get(\"level\", \"INFO\")\n",
    "        self.output = config.get(\"output\", \"console\")\n",
    "        self.log_levels = {\"DEBUG\": 0, \"INFO\": 1, \"WARNING\": 2, \"ERROR\": 3}\n",
    "        self.min_level = self.log_levels[self.level]\n",
    "\n",
    "    def _should_log(self, level: str) -> bool:\n",
    "        return self.log_levels[level] >= self.min_level\n",
    "\n",
    "    def _log(self, level: str, message: str):\n",
    "        if self._should_log(level):\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"{timestamp} [{level}] {message}\")\n",
    "\n",
    "    async def on_session_start(self, event: dict):\n",
    "        self._log(\"INFO\", \"=== Session Started ===\")\n",
    "\n",
    "    async def on_tool_invoked(self, event: dict):\n",
    "        tool_name = event.get(\"tool_name\", \"unknown\")\n",
    "        params = event.get(\"params\", {})\n",
    "        self._log(\"INFO\", f\"Tool invoked: {tool_name}\")\n",
    "        self._log(\"DEBUG\", f\"  Parameters: {params}\")\n",
    "\n",
    "    async def on_tool_completed(self, event: dict):\n",
    "        tool_name = event.get(\"tool_name\", \"unknown\")\n",
    "        success = event.get(\"success\", True)\n",
    "        symbol = \"âœ“\" if success else \"âœ—\"\n",
    "        self._log(\"INFO\", f\"Tool completed: {tool_name} {symbol}\")\n",
    "\n",
    "    async def on_tool_error(self, event: dict):\n",
    "        tool_name = event.get(\"tool_name\", \"unknown\")\n",
    "        error = event.get(\"error\", \"unknown error\")\n",
    "        self._log(\"ERROR\", f\"Tool error: {tool_name} - {error}\")\n",
    "\n",
    "    async def on_session_end(self, event: dict):\n",
    "        self._log(\"INFO\", \"=== Session Ended ===\")\n",
    "\n",
    "\n",
    "# Test with different log levels\n",
    "print(\"Demo: hooks-logging with INFO level:\")\n",
    "print(\"=\" * 60)\n",
    "hook_info = LoggingHook({\"level\": \"INFO\"})\n",
    "await hook_info.on_session_start({})\n",
    "await hook_info.on_tool_invoked({\"tool_name\": \"read_file\", \"params\": {\"path\": \"test.txt\"}})\n",
    "await hook_info.on_tool_completed({\"tool_name\": \"read_file\", \"success\": True})\n",
    "await hook_info.on_session_end({})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Demo: hooks-logging with DEBUG level:\")\n",
    "print(\"=\" * 60)\n",
    "hook_debug = LoggingHook({\"level\": \"DEBUG\"})\n",
    "await hook_debug.on_session_start({})\n",
    "await hook_debug.on_tool_invoked({\"tool_name\": \"read_file\", \"params\": {\"path\": \"test.txt\"}})\n",
    "await hook_debug.on_tool_completed({\"tool_name\": \"read_file\", \"success\": True})\n",
    "await hook_debug.on_session_end({})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Demo: hooks-logging with ERROR level (only errors):\")\n",
    "print(\"=\" * 60)\n",
    "hook_error = LoggingHook({\"level\": \"ERROR\"})\n",
    "await hook_error.on_session_start({})  # Won't log (INFO level)\n",
    "await hook_error.on_tool_invoked({\"tool_name\": \"read_file\", \"params\": {}})  # Won't log\n",
    "await hook_error.on_tool_error({\"tool_name\": \"read_file\", \"error\": \"File not found\"})  # Will log\n",
    "await hook_error.on_session_end({})  # Won't log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: hooks-approval - Human-in-the-Loop Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating hooks-approval behavior\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class ApprovalHook:\n",
    "    \"\"\"Simplified version of hooks-approval.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict[str, Any]):\n",
    "        self.auto_approve = config.get(\"auto_approve\", False)\n",
    "        self.timeout = config.get(\"timeout\", 300)\n",
    "\n",
    "    async def on_tool_invoked(self, event: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Intercept tool invocation and request approval.\n",
    "        Returns modified event with approval decision.\n",
    "        \"\"\"\n",
    "        tool_name = event.get(\"tool_name\", \"unknown\")\n",
    "        params = event.get(\"params\", {})\n",
    "\n",
    "        # Check if this tool requires approval\n",
    "        requires_approval = event.get(\"requires_approval\", False)\n",
    "\n",
    "        if not requires_approval:\n",
    "            # No approval needed\n",
    "            event[\"approved\"] = True\n",
    "            return event\n",
    "\n",
    "        if self.auto_approve:\n",
    "            print(f\"[AUTO-APPROVED] {tool_name}({params})\")\n",
    "            event[\"approved\"] = True\n",
    "            return event\n",
    "\n",
    "        # Request human approval (simulated)\n",
    "        print(\"\\nðŸ›‘ Approval Required:\")\n",
    "        print(f\"   Tool: {tool_name}\")\n",
    "        print(f\"   Parameters: {params}\")\n",
    "        print(\"   Approve? [y/n]: \", end=\"\")\n",
    "\n",
    "        # Simulate user response (in real implementation, would wait for input)\n",
    "        # For demo, we'll automatically approve after showing the prompt\n",
    "        response = \"y\"  # Simulated user input\n",
    "        print(response)\n",
    "\n",
    "        approved = response.lower() == \"y\"\n",
    "        event[\"approved\"] = approved\n",
    "\n",
    "        if approved:\n",
    "            print(\"   âœ“ Approved - proceeding...\")\n",
    "        else:\n",
    "            print(\"   âœ— Rejected - operation aborted\")\n",
    "\n",
    "        return event\n",
    "\n",
    "\n",
    "# Demo: Approval flow\n",
    "print(\"Demo: hooks-approval (manual mode)\")\n",
    "print(\"=\" * 60)\n",
    "approval_hook = ApprovalHook({\"auto_approve\": False})\n",
    "\n",
    "# Simulate tool invocation requiring approval\n",
    "event = {\"tool_name\": \"bash\", \"params\": {\"command\": \"rm important.txt\"}, \"requires_approval\": True}\n",
    "result = await approval_hook.on_tool_invoked(event)\n",
    "print(f\"\\nResult: approved={result['approved']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Demo: hooks-approval (auto-approve for dev)\")\n",
    "print(\"=\" * 60)\n",
    "approval_hook_auto = ApprovalHook({\"auto_approve\": True})\n",
    "result = await approval_hook_auto.on_tool_invoked(event)\n",
    "print(f\"Result: approved={result['approved']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: hooks-redaction - Secret Protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class RedactionHook:\n",
    "    \"\"\"Simplified version of hooks-redaction.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict[str, Any]):\n",
    "        self.enabled = config.get(\"enabled\", True)\n",
    "        self.replacement = config.get(\"replacement\", \"[REDACTED]\")\n",
    "\n",
    "        # Common secret patterns\n",
    "        default_patterns = [\n",
    "            r\"(sk-[a-zA-Z0-9]{48})\",  # API keys (Anthropic style)\n",
    "            r\"(sk-proj-[a-zA-Z0-9]{32,})\",  # OpenAI project keys\n",
    "            r\"(ghp_[a-zA-Z0-9]{36})\",  # GitHub personal access tokens\n",
    "            r'password[\"\\']?\\s*[:=]\\s*[\"\\']([^\"\\']+)[\"\\']',  # Password fields\n",
    "            r'api_key[\"\\']?\\s*[:=]\\s*[\"\\']([^\"\\']+)[\"\\']',  # API key fields\n",
    "        ]\n",
    "\n",
    "        # Combine default and user patterns\n",
    "        user_patterns = config.get(\"patterns\", [])\n",
    "        all_patterns = default_patterns + user_patterns\n",
    "\n",
    "        # Compile regex patterns\n",
    "        self.patterns = [re.compile(p) for p in all_patterns]\n",
    "\n",
    "    def redact_text(self, text: str) -> str:\n",
    "        \"\"\"Redact sensitive information from text.\"\"\"\n",
    "        if not self.enabled:\n",
    "            return text\n",
    "\n",
    "        result = text\n",
    "        for pattern in self.patterns:\n",
    "            result = pattern.sub(self.replacement, result)\n",
    "        return result\n",
    "\n",
    "    async def on_log_event(self, event: dict) -> dict:\n",
    "        \"\"\"Redact sensitive information from log events.\"\"\"\n",
    "        # Redact all string fields in the event\n",
    "        for key, value in event.items():\n",
    "            if isinstance(value, str):\n",
    "                event[key] = self.redact_text(value)\n",
    "            elif isinstance(value, dict):\n",
    "                # Recursively redact nested dicts\n",
    "                event[key] = await self.on_log_event(value)\n",
    "        return event\n",
    "\n",
    "\n",
    "# Demo: Redaction in action\n",
    "print(\"Demo: hooks-redaction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "redaction_hook = RedactionHook({\"enabled\": True})\n",
    "\n",
    "# Test cases with secrets\n",
    "test_cases = [\n",
    "    \"Using API key sk-ant-abc123def456ghi789jkl012mno345pqr678stu901vwx\",\n",
    "    'Config: {\"api_key\": \"sk-proj-abcdefghijklmnopqrstuvwxyz123456\"}',\n",
    "    \"GitHub token: ghp_1234567890abcdefghijklmnopqrstuv\",\n",
    "    \"Database password: 'super_secret_password_123'\",\n",
    "    \"No secrets here, just regular text\",\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"  Before: {test}\")\n",
    "    redacted = redaction_hook.redact_text(test)\n",
    "    print(f\"  After:  {redacted}\")\n",
    "    if test != redacted:\n",
    "        print(\"  âœ“ Redaction applied\")\n",
    "    else:\n",
    "        print(\"  â†’ No secrets detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: hooks-backup - Automatic State Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class BackupHook:\n",
    "    \"\"\"Simplified version of hooks-backup.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict[str, Any]):\n",
    "        self.backup_dir = Path(config.get(\"backup_dir\", \".amplifier/backups\"))\n",
    "        self.interval = config.get(\"interval\", 300)  # 5 minutes\n",
    "        self.max_backups = config.get(\"max_backups\", 10)\n",
    "        self.backup_on_end = config.get(\"backup_on_end\", True)\n",
    "        self.last_backup_time = 0\n",
    "\n",
    "    def _should_backup(self) -> bool:\n",
    "        \"\"\"Check if it's time for a periodic backup.\"\"\"\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.last_backup_time\n",
    "        return elapsed >= self.interval\n",
    "\n",
    "    def _save_backup(self, session_data: dict) -> str:\n",
    "        \"\"\"Save backup to file.\"\"\"\n",
    "        # Generate backup filename with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        session_id = session_data.get(\"session_id\", \"unknown\")\n",
    "        filename = f\"backup_{session_id}_{timestamp}.json\"\n",
    "\n",
    "        # In real implementation, would write to disk\n",
    "        # self.backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # filepath = self.backup_dir / filename\n",
    "        # filepath.write_text(json.dumps(session_data, indent=2))\n",
    "\n",
    "        print(f\"ðŸ’¾ Backup saved: {filename}\")\n",
    "        print(f\"   Messages: {len(session_data.get('messages', []))}\")\n",
    "        print(f\"   Size: {len(json.dumps(session_data))} bytes\")\n",
    "\n",
    "        self.last_backup_time = time.time()\n",
    "        return filename\n",
    "\n",
    "    async def on_message_added(self, event: dict):\n",
    "        \"\"\"Periodic backup when messages are added.\"\"\"\n",
    "        if self._should_backup():\n",
    "            session_data = event.get(\"session_data\", {})\n",
    "            self._save_backup(session_data)\n",
    "\n",
    "    async def on_session_end(self, event: dict):\n",
    "        \"\"\"Save final backup when session ends.\"\"\"\n",
    "        if self.backup_on_end:\n",
    "            session_data = event.get(\"session_data\", {})\n",
    "            print(\"\\nðŸ“¦ Final backup on session end:\")\n",
    "            self._save_backup(session_data)\n",
    "\n",
    "\n",
    "# Demo: Backup behavior\n",
    "print(\"Demo: hooks-backup\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "backup_hook = BackupHook(\n",
    "    {\n",
    "        \"backup_dir\": \".amplifier/backups\",\n",
    "        \"interval\": 5,  # 5 seconds for demo (normally 300)\n",
    "        \"max_backups\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Simulate session activity\n",
    "session_data = {\"session_id\": \"demo-123\", \"messages\": []}\n",
    "\n",
    "print(\"Simulating conversation with periodic backups...\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    # Add a message\n",
    "    session_data[\"messages\"].append({\"role\": \"user\", \"content\": f\"Message {i + 1}\"})\n",
    "\n",
    "    # Trigger backup check\n",
    "    await backup_hook.on_message_added({\"session_data\": session_data})\n",
    "\n",
    "    # Wait to trigger interval\n",
    "    if i < 2:  # Don't wait after last iteration\n",
    "        print(\"   Continuing conversation...\\n\")\n",
    "        await asyncio.sleep(6)  # Wait 6 seconds (> 5 second interval)\n",
    "\n",
    "# End session\n",
    "await backup_hook.on_session_end({\"session_data\": session_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Event System Deep Dive\n",
    "\n",
    "Understanding the event types and their data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Standard Event Catalog\n\n**Session Lifecycle**\n- `session:start` - Session begins\n  - Data: session_id, timestamp\n- `session:end` - Session ends\n  - Data: session_id, timestamp, duration\n- `session:error` - Session error occurs\n  - Data: session_id, error, traceback\n\n**Tool Execution**\n- `tool:invoked` - Tool called by LLM\n  - Data: tool_name, params, tool_call_id\n- `tool:completed` - Tool finished successfully\n  - Data: tool_name, result, duration\n- `tool:error` - Tool execution failed\n  - Data: tool_name, error\n- `tool:approval:requested` - Approval needed\n  - Data: tool_name, params\n\n**Agent Activity**\n- `agent:spawned` - Child agent started\n  - Data: agent_name, task, parent_session\n- `agent:completed` - Child agent finished\n  - Data: agent_name, result, duration\n\n**Context Management**\n- `context:compact:start` - Compaction beginning\n  - Data: token_count, message_count\n- `context:compact:end` - Compaction finished\n  - Data: old_tokens, new_tokens, reduction\n- `context:usage:warning` - Usage threshold crossed\n  - Data: percent, tokens, limit\n\n**LLM Provider**\n- `llm:request:debug` - LLM request (DEBUG level)\n  - Data: provider, model, message_count\n- `llm:response:debug` - LLM response (DEBUG level)\n  - Data: provider, tokens, tool_calls"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Creating a Custom Hook\n",
    "\n",
    "Let's build a custom hook that monitors tool execution time and warns about slow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom hook: Performance monitoring\n",
    "class PerformanceMonitorHook:\n",
    "    \"\"\"Monitor tool execution performance and warn about slow operations.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict[str, Any]):\n",
    "        self.slow_threshold = config.get(\"slow_threshold\", 5.0)  # seconds\n",
    "        self.very_slow_threshold = config.get(\"very_slow_threshold\", 10.0)\n",
    "        self.track_history = config.get(\"track_history\", True)\n",
    "        self.tool_times: dict[str, list[float]] = {}\n",
    "\n",
    "    async def on_tool_completed(self, event: dict):\n",
    "        \"\"\"Monitor tool completion time.\"\"\"\n",
    "        tool_name = event.get(\"tool_name\", \"unknown\")\n",
    "        duration = event.get(\"duration\", 0.0)\n",
    "\n",
    "        # Track history\n",
    "        if self.track_history:\n",
    "            if tool_name not in self.tool_times:\n",
    "                self.tool_times[tool_name] = []\n",
    "            self.tool_times[tool_name].append(duration)\n",
    "\n",
    "        # Check thresholds\n",
    "        if duration >= self.very_slow_threshold:\n",
    "            print(f\"âš ï¸  VERY SLOW: {tool_name} took {duration:.2f}s (threshold: {self.very_slow_threshold}s)\")\n",
    "        elif duration >= self.slow_threshold:\n",
    "            print(f\"â±ï¸  SLOW: {tool_name} took {duration:.2f}s (threshold: {self.slow_threshold}s)\")\n",
    "        else:\n",
    "            print(f\"âœ“ {tool_name} completed in {duration:.2f}s\")\n",
    "\n",
    "        # Show average if we have history\n",
    "        if self.track_history and len(self.tool_times[tool_name]) > 1:\n",
    "            avg = sum(self.tool_times[tool_name]) / len(self.tool_times[tool_name])\n",
    "            print(f\"   Average for {tool_name}: {avg:.2f}s ({len(self.tool_times[tool_name])} calls)\")\n",
    "\n",
    "    async def on_session_end(self, event: dict):\n",
    "        \"\"\"Show performance summary.\"\"\"\n",
    "        if not self.track_history:\n",
    "            return\n",
    "\n",
    "        print(\"\\nðŸ“Š Performance Summary:\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for tool_name, times in sorted(self.tool_times.items()):\n",
    "            avg = sum(times) / len(times)\n",
    "            min_time = min(times)\n",
    "            max_time = max(times)\n",
    "            total = sum(times)\n",
    "\n",
    "            print(f\"\\n{tool_name}:\")\n",
    "            print(f\"  Calls: {len(times)}\")\n",
    "            print(f\"  Average: {avg:.2f}s\")\n",
    "            print(f\"  Min: {min_time:.2f}s\")\n",
    "            print(f\"  Max: {max_time:.2f}s\")\n",
    "            print(f\"  Total: {total:.2f}s\")\n",
    "\n",
    "\n",
    "# Demo: Performance monitoring\n",
    "print(\"Demo: Custom Performance Monitor Hook\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "perf_hook = PerformanceMonitorHook({\"slow_threshold\": 3.0, \"very_slow_threshold\": 5.0, \"track_history\": True})\n",
    "\n",
    "# Simulate various tool executions\n",
    "tool_executions = [\n",
    "    {\"tool_name\": \"read_file\", \"duration\": 0.5},\n",
    "    {\"tool_name\": \"bash\", \"duration\": 3.5},  # Slow\n",
    "    {\"tool_name\": \"read_file\", \"duration\": 0.3},\n",
    "    {\"tool_name\": \"web_fetch\", \"duration\": 6.2},  # Very slow\n",
    "    {\"tool_name\": \"bash\", \"duration\": 2.1},\n",
    "    {\"tool_name\": \"read_file\", \"duration\": 0.4},\n",
    "]\n",
    "\n",
    "print(\"\\nSimulating tool executions:\\n\")\n",
    "for exec_event in tool_executions:\n",
    "    await perf_hook.on_tool_completed(exec_event)\n",
    "    print()\n",
    "\n",
    "# Show summary\n",
    "await perf_hook.on_session_end({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging the Custom Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to package the custom hook as a module\n",
    "\n",
    "mount_function_example = '''\n",
    "# amplifier_module_hooks_performance/__init__.py\n",
    "\n",
    "async def mount(coordinator, config: dict | None = None):\n",
    "    \"\"\"\n",
    "    Mount the performance monitoring hook.\n",
    "\n",
    "    Args:\n",
    "        coordinator: Session coordinator\n",
    "        config: Hook configuration\n",
    "\n",
    "    Returns:\n",
    "        Optional cleanup function\n",
    "    \"\"\"\n",
    "    config = config or {}\n",
    "\n",
    "    # Create hook instance\n",
    "    hook = PerformanceMonitorHook(config)\n",
    "\n",
    "    # Register event handlers\n",
    "    @coordinator.hooks.on(\"tool:completed\")\n",
    "    async def on_tool_completed(event):\n",
    "        await hook.on_tool_completed(event)\n",
    "\n",
    "    @coordinator.hooks.on(\"session:end\")\n",
    "    async def on_session_end(event):\n",
    "        await hook.on_session_end(event)\n",
    "\n",
    "    # Optionally declare observable events\n",
    "    obs_events = coordinator.get_capability(\"observability.events\") or []\n",
    "    obs_events.extend([\n",
    "        \"performance:slow_tool\",\n",
    "        \"performance:very_slow_tool\"\n",
    "    ])\n",
    "    coordinator.register_capability(\"observability.events\", obs_events)\n",
    "\n",
    "    logger.info(\"Performance monitor hook mounted\")\n",
    "\n",
    "    # No cleanup needed\n",
    "    return None\n",
    "'''\n",
    "\n",
    "pyproject_example = \"\"\"\n",
    "# pyproject.toml\n",
    "\n",
    "[project]\n",
    "name = \"amplifier-module-hooks-performance\"\n",
    "version = \"1.0.0\"\n",
    "dependencies = [\n",
    "    \"amplifier-core>=1.0.0\"\n",
    "]\n",
    "\n",
    "[project.entry-points.\"amplifier.modules\"]\n",
    "hooks-performance = \"amplifier_module_hooks_performance:mount\"\n",
    "\"\"\"\n",
    "\n",
    "profile_usage = \"\"\"\n",
    "# profiles/dev.md\n",
    "---\n",
    "hooks:\n",
    "  - module: hooks-performance\n",
    "    source: git+https://github.com/org/amplifier-module-hooks-performance@main\n",
    "    config:\n",
    "      slow_threshold: 3.0\n",
    "      very_slow_threshold: 5.0\n",
    "      track_history: true\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "print(\"Packaging Custom Hook as Module\\n\" + \"=\" * 60)\n",
    "print(\"\\n1. mount() function:\")\n",
    "print(mount_function_example)\n",
    "print(\"\\n2. pyproject.toml:\")\n",
    "print(pyproject_example)\n",
    "print(\"\\n3. Usage in profile:\")\n",
    "print(profile_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Hook Composition - Layering Hooks\n",
    "\n",
    "Hooks work together to provide \"defense in depth\". Let's see how multiple hooks compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating multiple hooks working together\n",
    "class HookOrchestrator:\n",
    "    \"\"\"Manages multiple hooks and coordinates event dispatch.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.hooks: list = []\n",
    "\n",
    "    def register(self, hook: Any):\n",
    "        \"\"\"Register a hook.\"\"\"\n",
    "        self.hooks.append(hook)\n",
    "\n",
    "    async def emit(self, event_name: str, event_data: dict):\n",
    "        \"\"\"Emit event to all registered hooks.\"\"\"\n",
    "        print(f\"\\nðŸ“¡ Event: {event_name}\")\n",
    "\n",
    "        for i, hook in enumerate(self.hooks, 1):\n",
    "            # Check if hook handles this event\n",
    "            handler_name = f\"on_{event_name.replace(':', '_')}\"\n",
    "            if hasattr(hook, handler_name):\n",
    "                handler = getattr(hook, handler_name)\n",
    "                print(f\"  â””â”€ Hook {i} ({hook.__class__.__name__}): processing...\")\n",
    "                result = await handler(event_data)\n",
    "                # Some hooks modify events\n",
    "                if result is not None:\n",
    "                    event_data = result\n",
    "\n",
    "        return event_data\n",
    "\n",
    "\n",
    "# Demo: Multiple hooks working together\n",
    "print(\"Demo: Hook Composition (Defense in Depth)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator = HookOrchestrator()\n",
    "\n",
    "# Register multiple hooks in order\n",
    "hooks_config = [\n",
    "    (\"Redaction\", RedactionHook({\"enabled\": True})),\n",
    "    (\"Logging\", LoggingHook({\"level\": \"INFO\"})),\n",
    "    (\"Approval\", ApprovalHook({\"auto_approve\": False})),\n",
    "    (\"Backup\", BackupHook({\"backup_dir\": \".amplifier/backups\", \"interval\": 300})),\n",
    "]\n",
    "\n",
    "print(\"\\nRegistered hooks (in order):\")\n",
    "for name, hook in hooks_config:\n",
    "    orchestrator.register(hook)\n",
    "    print(f\"  {len(orchestrator.hooks)}. {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Simulating tool execution with all hooks:\\n\")\n",
    "\n",
    "# Simulate tool invocation\n",
    "tool_event = {\n",
    "    \"tool_name\": \"bash\",\n",
    "    \"params\": {\"command\": \"echo 'API key: sk-ant-abc123def456ghi789jkl012mno345pqr678stu901vwx'\"},\n",
    "    \"requires_approval\": True,\n",
    "}\n",
    "\n",
    "result = await orchestrator.emit(\"tool:invoked\", tool_event)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"What happened:\")\n",
    "print(\"  1. Redaction: Removed API key from event data\")\n",
    "print(\"  2. Logging: Logged the redacted event\")\n",
    "print(\"  3. Approval: Requested human approval\")\n",
    "print(\"  4. Backup: (Would save state after approval)\")\n",
    "print(\"\\nThis is 'defense in depth' - multiple layers of protection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Best Practices Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Hooks Best Practices\n\n**1. Always Enable Logging**\n- Why: Visibility is essential for debugging and monitoring\n- Dev: level: DEBUG, output: console\n- Prod: level: WARNING, output: file\n\n**2. Layer Hooks for Defense**\n- Why: Multiple hooks provide defense in depth\n- Layers:\n  - Redaction â†’ protect secrets\n  - Logging â†’ know what happened\n  - Approval â†’ prevent mistakes\n  - Backup â†’ enable recovery\n\n**3. Order Matters**\n- Why: Hooks process events in registration order\n- Rule: Modifying hooks first, observing hooks last\n- Example: hooks-redaction before hooks-logging\n\n**4. Configure by Environment**\n- Why: Different environments need different settings\n- Dev: Verbose logging, auto-approve, no redaction\n- Staging: Info logging, manual approve, redaction on\n- Prod: Warning logging, manual approve, redaction on, backup on\n\n**5. Use Auto-Discovery**\n- Why: Automatically handle module events\n- Benefit: Zero configuration, forwards compatible\n- Default: auto_discover: true in hooks-logging\n\n**6. Monitor Context Usage**\n- Why: Prevent unexpected compaction or API errors\n- Hook: hooks-status-context\n- Thresholds: warn: 80%, alert: 90%\n\n**7. Never Block Execution**\n- Why: Hooks should not hang the system\n- Rule: Fast operations only, use timeouts\n- Exception: Approval hooks (intentional pause)\n\n**8. Handle Errors Gracefully**\n- Why: Hook failure shouldn't crash session\n- Pattern: try/except in all handlers\n- Fallback: Log error and continue"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Quick Reference - Choosing Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Hook Selection Guide\n\n**Decision Tree**\n\n- **Need visibility?** â†’ hooks-logging (Always!)\n- **Need recovery?** â†’ hooks-backup (Production)\n- **Need human approval?** â†’ hooks-approval (High-risk operations)\n- **Need secret protection?** â†’ hooks-redaction (Compliance/production)\n- **Need cost control?** â†’ hooks-scheduler-cost-aware (Budget constraints)\n- **Need smart routing?** â†’ hooks-scheduler-heuristic (Multi-provider setups)\n- **Need context monitoring?** â†’ hooks-status-context (Long conversations)\n- **Need better UX?** â†’ hooks-streaming-ui (Interactive CLI/web)\n- **Need todo tracking?** â†’ hooks-todo-reminder (Complex multi-step tasks)\n\n**Recommended Hook Stack by Environment**\n\n**Development:**\n- hooks-logging (level: DEBUG)\n- hooks-approval (auto_approve: true)\n\n**Staging:**\n- hooks-logging (level: INFO)\n- hooks-backup (interval: 300)\n- hooks-approval (auto_approve: false)\n- hooks-status-context\n\n**Production:**\n- hooks-redaction (enabled: true)\n- hooks-logging (level: WARNING, output: file)\n- hooks-backup (interval: 300, max_backups: 50)\n- hooks-approval (timeout: 600)\n- hooks-status-context"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. **Hook Philosophy**: Observability mechanism, event-driven, composable\n",
    "2. **Event Lifecycle**: From emission to handler execution\n",
    "3. **Available Hooks**: 9 built-in hooks for different purposes\n",
    "4. **hooks-logging**: Essential observability (always enable!)\n",
    "5. **hooks-approval**: Human-in-the-loop control\n",
    "6. **hooks-redaction**: Automatic secret protection\n",
    "7. **hooks-backup**: State preservation and recovery\n",
    "8. **Event System**: Standard events and custom events\n",
    "9. **Custom Hooks**: Building your own (performance monitor example)\n",
    "10. **Hook Composition**: Layering for defense in depth\n",
    "11. **Best Practices**: Configuration, ordering, error handling\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Hooks are the agent's nervous system** - they sense and respond\n",
    "- **Always use hooks-logging** - visibility is essential\n",
    "- **Layer hooks for defense in depth** - multiple protections\n",
    "- **Order matters** - modifying hooks before observing hooks\n",
    "- **Configure by environment** - different needs for dev/prod\n",
    "- **Auto-discovery is powerful** - zero-config event handling\n",
    "- **Custom hooks are easy** - just implement event handlers\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Enable hooks-logging in all profiles\n",
    "2. Add hooks-backup for production\n",
    "3. Use hooks-approval for high-risk operations\n",
    "4. Create custom hooks for your needs\n",
    "5. Layer hooks for comprehensive coverage\n",
    "\n",
    "### Related Documentation\n",
    "\n",
    "- [Modules Guide](./guides/modules.md) - Module system overview\n",
    "- [Mounts Guide](./guides/mounts.md) - How hooks are loaded\n",
    "- [Development Guide](./guides/development.md) - Creating custom hooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}